---
title: "Exemplo processo gaussiano"
author: "Douglas Braga"
date: "18 de outubro de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
### From this post: http://stats.stackexchange.com/q/232959/67822

set.seed(0)
n = 50 #No. test points
Xtest = seq(-5, 5, length.out = n)   #Test points: 50 points between -5 and +5: -5, -4.79, -4.59,..., 4.59, 4.79, 5

kernel = function(a,b, param){                                   
  # Defining a function the Gaussian process is exp{-1/2 abs(x_1 - x_2)^2}
  #GP squared exponential kernel:
  #Leaving aside the abs value, (x_1 - x_2)^2 = a^2 + b^2 - 2 ab. And making the matrices congruous:
  sqdist = outer(a^2,b^2,FUN=`+`) - 2 * (a %*% t(b))
  exp(-.5 * (1/param) * sqdist) # This is the kernel: when distance is inf. the exponential becomes 1/e^inf =0, when dist=0, k =1
}
  
param = 0.1                                        
K_test = kernel(Xtest, Xtest, param)                       
#Kernel at test points: all the points against each other.
  
# Draw samples from the prior at our test points:
  
Ch_test = chol(K_test + 1e-15 * diag(n)) # Square root of the kernel values (the Cholesky)
m = matrix(rnorm(n * 3), ncol = 3)
f_prior = t(m) %*% Ch_test # Generating multivariate normals through the Cholesky representing the kernels


plot(Xtest,f_prior[1,], type="l", col='darkorange', ylim=c(-2.2,2.2), lwd = 2,
     xlab ="", ylab ="")
title(main="Three samples from the GP Prior", cex.main=0.85)
abline(h = 0)
colors=c(2, "darkred","blue")
for(i in 2:3){
lines(Xtest, f_prior[i,], type = 'l', 
      col=colors[i], lwd=2) # Plotting
}
```

```{r}
### From this post: http://stats.stackexchange.com/q/232959/67822

# Noiseless training data:
Xtrain = c(-4,-3, -2, -1, 1)
ytrain = sin(Xtrain)

# Apply the kernel function to our training points:
K_train = kernel(Xtrain, Xtrain, param)

Ch_train = chol(K_train + 0.00005 * diag(length(Xtrain)))

# Compute the mean at our test points:

K_trte = kernel(Xtrain, Xtest, param)
core = solve(Ch_train) %*% K_trte
temp = solve(Ch_train) %*% ytrain
mu = t(core) %*% temp

# Compute the standard deviation:

tempor = colSums(core^2)

# Notice that all.equal(diag(t(Lk) %*% Lk), colSums(Lk^2)) TRUE

s2 = diag(K_test) - tempor
stdv = sqrt(s2)

# Draw samples from the posterior at our test points:

Ch_post_gener = chol(K_test + 1e-6 * diag(n) - (t(core) %*% core))
m_prime = matrix(rnorm(n * 3), ncol = 3)
sam = Ch_post_gener %*% m_prime
f_post = as.vector(mu) + sam



colors=c(2, "darkred","blue")
plot(Xtest,f_post[,1], type="l", lwd = 2, col='darkorange', 
     ylim=c(-2.5,2.5),
     xlab='',ylab='') 
title(main="Three samples from the GP Posterior
      Training points added",
      cex.main=0.85)

abline(h = 0)

for(i in 2:3){
  lines(Xtest, f_post[,i], type = 'l', lwd=2, col=colors[i]) # Plotting
}
points(Xtrain, ytrain, pch = 20, cex=2)
```

```{r}
### From this post: http://stats.stackexchange.com/q/232959/67822

plot(Xtest,mu, type="l", lwd=2, xlim= c(-5,2), ylim=c(-2.5,2.5),
     xlab='',ylab='')
title(main = "Estimated mean and 2 SD's
      with training points", cex.main=0.85,
      sub= "true source of data (sin curve) in yellow",
      cex.sub=0.7)

abline(h = 0)


polygon(c(Xtest,rev(Xtest)),c(mu-2*stdv,rev(mu+2*stdv)),
        col =rgb(0,0,0.5,0.3) ,border = FALSE)

lines(Xtest, mu + 2 * stdv, col="red",lty=2)
lines(Xtest, mu - 2 * stdv, col="red",lty=2)
points(Xtrain, ytrain, pch = 19, cex=.8)

lines(Xtest, sin(Xtest), col="yellow",lty=3, lwd=3)
```

