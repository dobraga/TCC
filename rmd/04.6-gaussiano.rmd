## Função de covariância

As funções de covariância ou kernel é uma função de dois argumentos, $K(x, x')$, normalmente é uma função simétrica, ou seja, $K(x, x') = K(x', x)$, não negativa e pode ser interpretada como uma medida de similaridade^[@robert2014machine], a seguir serão mostradas os kernels mais usuais.

1. Exponencial ao quadrado: $K_{SE}(d) = exp(-\frac{|d|^2}{2l^2})$

2. Matern: $K_{Matern}(d) = \frac{2^{(1-\nu)}}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu}r}{l} \right)^\nu K_v \left( \frac{\sqrt{2\nu}|d|}{l}\right)$

3. Gaussiano: $K_G(d) = \sigma^2 e^{\frac{-1}{2}(\frac{d}{l})^2}$

Onde $d=x-x'$, os parâmetros $\nu$ e l são positivos e $K_v$ é a função de Bessel modificada.

## Processos Gaussianos

Pode-se imaginar como exemplo $n$ ponto gerados de uma distribuição n-dimensional normal multivariada, agora imaginando infinitos pontos de um domínio contínuo tem-se um processo gaussiano sendo a generalização infinito-dimensional de uma variável gaussiana^[@williams1996gaussian] é comumente são utilizados para interpolação de pontos não linear, sendo assim, o objetivo é prever pontos no espaço não observados, levando em conta que pontos próximos aos observados não devem mudar bruscamente o valor da variável de interesse. 

Abaixo será apresentado um exemplo.

![[Retirada do livro de Kevin Murphy.\label{gp}](https://github.com/probml/pmtk3/blob/30d7a1952f3979b16e92dbfa4cd1ce0e402cf7d8/docs/demoOutput/bookDemos/(15)-Gaussian_processes/gprDemoNoiseFree_02.png)](fig/gp.png){height=30%}  

Na figura \ref{gp} pode-se observar, que neste caso, esta sendo considerado que não há incerteza de medição nos pontos observados representados pelo $x$ no gráfico, as bandas representam um intervalo de confiança de 95%.

Um processo gaussiano é totalmente definido por uma função de média($m(x)$) e por um função de covariância($K(x, x')$) do processo real($f(x)$).

$$
f(x) \sim GP(m(x), K(x, x'))
$$

### Predição sem erro

Suponha uma base de treino $\mathcal{D} = \{(x_i, f_i), i=1:N\}$, onde $f_i = f(x_i)$ é uma observação sem erro de avaliação, e uma outra base que tenhamos apenas $\boldsymbol{X}_*$ para que seja encontrado $f_*$, ou seja, queremos prever os valores de $f_*$ fazendo uma interpolação com os dados conhecido.

$$
\begin{pmatrix} f\\f_*   \end{pmatrix} = N\begin{pmatrix} \begin{pmatrix} \mu \\ \mu_* \end{pmatrix}, \begin{pmatrix} \boldsymbol{K} & \boldsymbol{K_*} \\ \boldsymbol{K_*}^T & \boldsymbol{K_{**}}  \end{pmatrix}\end{pmatrix}
$$

### Predição com erro

### Otimização Bayesiana

A técnica de otimização bayesiana é uma das abordagens mais eficientes em termos do número de avaliações de funções necessárias, além disto, é útil quando a avaliação da função tem custo alto, quando não é conhecido as derivadas da função em análise, quando a função não é convexa ou até mesmo desconhecida.

É conhecida com este nome por ter como base o teorema de Bayes, visto anteriormente, que diz que a posteriori de um modelo(M) dada evidência dos dados(E) é proporcional à verossimilhança de E dado M vezes a priori do modelo(M):

$$
P(M|E) \propto P(E|M)P(M)
$$

A optimização bayesiana utiliza como distribuição a priori o Processo Gaussiano com o vetor de médias 0 para facilitar as contas, porém, um fato interessante do processo gaussiano é que a média pode ser totalmente definida pela sua matriz de covariância.^[@bishop2012pattern]

Será assumido que a função $f(x)$ é uma observação de um processo gaussiano e que as observações, $(x_n,y_n)_{n=1}^N$, onde $y_n \mbox{~} N(f(x_n),\nu)$ e $\nu$ é a variância dos ruídos introduzidos na função de observação. Utilizando a regra de bayes para combinar a priori com os dados para gerar a posteriori que determina qual ponto de $\chi$.

Na optimização bayesiana, a priori que é definida está ligada ao espaço que acredita-se que a função de custo possa variar. Mesmo que esta função seja desconhecida, é razoável supor que exista conhecimento prévio sobre alguma de suas propriedades, como a suavidade, isto torna algumas funções objetivas mais prováveis que outras.

Define-se $x_i$ sendo a i-ésima amostra, e $f(x_i)$ a observação da função objetiva em $x_i$. Serão acumulados as observações $D_{i:t}=\left\{ x_{i:t},f(x_{1:t}) \right\}$, a distribuição apriori será combinada com a verossimilhança $P(D_{i:t}|f)$, que para facilitar, pode-se pensar que a verossimilhança é a probabilidade do que vimos ocorrer dado que o que achamos sobre a função é a realidade. Se a nossa crença anterior é que a função objetiva é muito suave e isenta de erros, os dados com alta variância ou oscilações devem ser considerados menos prováveis do que os dados que mal se desviam da média. Agora, podemos combiná-los para obter nossa distribuição aposteriori:

$$
P(f|D_{1:t}) \propto P(D_{1:t}|f)P(f)
$$

![Um exemplo do uso da otimização bayesiana.\label{bo}](fig/bo.png){height=60%}

A figura \ref{bo} mostram uma aproximação do processo Gaussiano (GP) da função objetivo sobre quatro iterações dos valores amostrados da função objetivo. A figura também mostra o função de aquisição nas parcelas sombreadas mais baixas. A aquisição é alta onde o GP
prevê um objetivo elevado (exploração) e onde a incerteza de previsão é alta (exploração) - áreas com os dois atributos são amostradas primeiro. Note que a área no extrema esquerda permanece sem amostragem, como enquanto ele tem alta incerteza, é (corretamente) previsto
para oferecer pouca melhoria sobre a mais alta observação.

A posteriori captura a apriori atualizada da função objetivo. Para que uma amostra eficiente seja obtida é utilizado uma função de aquisição para determinar o próximo ponto $x_{t+1} \in A$.  A decisão representa um trade-off automático entre exploração (onde a função objetivo é muito incerta) e exploração (tentando valores de $x$ onde a função objetivo deve ser alta). A optimização bayesiana tem como objetivo minimizar a quantidade de interações necessárias para localizar o ponto máximo e também se comporte bem em funções com múltiplos pontos de máximo.

A figura \ref{bo} representa um uma optimização de uma dimensão, onde é iniciado com dois ponto aleatórios. A cada iteração, a função de aquisição é maximizada para determinar o próximo ponto para a amostra da função objetivo. A função de aquisição leva em consideração a média a posteriori calculada para os pontos e a variância da previsão. E então é amostrado o $\operatorname{argmax}$ da função da função de aquisição, o processo gaussiano é atualizado e o processo é repetido para os próximos passos.
